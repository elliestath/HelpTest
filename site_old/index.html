<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="None">
  
  <link rel="shortcut icon" href="img/favicon.ico">
  <title>HELP for PhotoMatch - PhotoMatch</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>
  <link href='https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="css/theme.css" type="text/css" />
  <link rel="stylesheet" href="css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "HELP for PhotoMatch";
    var mkdocs_page_input_path = "index.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="js/jquery-2.1.1.min.js" defer></script>
  <script src="js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="." class="icon icon-home"> PhotoMatch</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="tocbase current">
    
    
      


  
    
    <li class="navtree toctree-l1 page current">
      <a class="current" href=".">
        HELP for PhotoMatch
      </a>
    </li>
    
      



  <li class="toctree-l1 current">
    <ul class="subnav-l1 current">
    
      
          

  <li class="toctree-l2 current with-children">
    <a href="#preprocessing">
      Preprocessing
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2 current">
    <ul class="subnav-l2 current">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#decolorization">Decolorization</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#acebsf">ACEBSF</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#clahe">CLAHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#cmbfhe">CMBFHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#dhe">DHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#fahe">FAHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#hmclahe">HMCLAHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#lce-bsescs">LCE-BSESCS</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#msrcp">MSRCP</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#noshp">NOSHP</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#pohe">POHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#rswhe">RSWHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#wallis-filter">Wallis Filter</a>
        </li>
    
    </ul>
  </li>

      
    
      
          

  <li class="toctree-l2">
    <a href="#feature-extraction">
      Feature Extraction
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2">
    <ul class="subnav-l2 toc-hidden">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#keypoint-detectors">KEYPOINT DETECTORS</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#agast">AGAST</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#akaze">AKAZE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#brisk">BRISK</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#fast">FAST</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#gftt">GFTT</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#kaze">KAZE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#msd">MSD</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#mser">MSER</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#orb">ORB</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#sift">SIFT</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#star">STAR</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#surf">SURF</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#keypoint-descriptors">KEYPOINT DESCRIPTORS</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#akaze_1">AKAZE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#brief">BRIEF</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#brisk_1">BRISK</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#daisy">DAISY</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#freak">FREAK</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#hog">HOG</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#kaze_1">KAZE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#latch">LATCH</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#lucid">LUCID</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#lss">LSS</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#orb_1">ORB</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#sift_1">SIFT</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#surf_1">SURF</a>
        </li>
    
    </ul>
  </li>

      
    
      
          

  <li class="toctree-l2">
    <a href="#feature-matching">
      Feature Matching
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2">
    <ul class="subnav-l2 toc-hidden">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#brute-force">Brute Force</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#flann">FLANN</a>
        </li>
    
    </ul>
  </li>

      
    
      
          

  <li class="toctree-l2">
    <a href="#quality-control">
      Quality Control
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2">
    <ul class="subnav-l2 toc-hidden">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#roc-curves">ROC Curves</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#det-curves">DET Curves</a>
        </li>
    
    </ul>
  </li>

      
    
    </ul>
  </li>


  
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="index_old/">HELP for PhotoMatch</a>
  </li>
    
  </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href=".">PhotoMatch</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".">Docs</a> &raquo;</li>
    
      
    
    <li>HELP for PhotoMatch</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="help-for-photomatch">HELP for PhotoMatch</h1>
<p>Brief explanations of the algorithms used in the scientific initiative PhotoMatch. </p>
<h2 id="preprocessing">Preprocessing</h2>
<p>This step offers the option to use image enhancement techniques that could potentially facilitate the detection of robust image features.</p>
<p><strong>PARAMETERS</strong>
Full Image Size/Max Image Size</p>
<h3 id="decolorization">Decolorization</h3>
<p>Contrast Preserving Decolorization</p>
<h3 id="acebsf">ACEBSF</h3>
<p><em>Adaptive Contrast Enhancement Based on modified Sigmoid Function.</em> 
<em>Parameters</em>
 - Blocksize (Width: 8/Height: 8)
 - L: 0.3
 - K1: 10
 - K2: 0.50</p>
<p>[Source] <em>S. Lal and M. Chandra, "Efficient algorithm for contrast enhancement of natural images," The International Arab Journal of Information Technology, vol. 11, no. 1, January 2014.</em></p>
<h3 id="clahe">CLAHE</h3>
<p><em>Contrast Limited Adaptive Histogram Equalization.</em>
CLAHE is a local contrast enhancement technique robust to outliers. Local contrast enhancement methods such as adaptive histogram equalization (AHE) divide the original image into several non-overlapped sub-blocks and apply histogram equalization accordingly. CLAHE is an improvement of AHE and performs well on low contrast images.
<em>Parameters</em>
 - Clip Limit: 40
 - Tiles Size X: 8
 - Tiles Size Y: 8</p>
<p>[Source] (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3092044/) </p>
<h3 id="cmbfhe">CMBFHE</h3>
<p>Cascaded Multistep Binomial Filtering Histogram Equalization.
<em>Parameters</em>
 - Block Size (Width: 11/Height: 11)</p>
<p>Source: <em>F. Lamberti, B. Montrucchio, and A. Sanna, "CMBFHE: a novel contrast enhancement technique based on cascaded multistep binomial filtering histogram equalization," TCE, vol. 52, no. 3, 2006.</em></p>
<h3 id="dhe">DHE</h3>
<p>Dynamic Histogram Equalization.
<em>Parameters</em>
 - x: 1</p>
<p>Source: <em>M. Abdullah-Al-Wadud, Md. Hasanul Kabir, M. Ali Akber Dewan, and O. Chae, "A dynamic histogram equalization for image contrast enhancement," Intl. Conf. Consumer Electronics, pp. 1-2, 2007.</em></p>
<h3 id="fahe">FAHE</h3>
<p>Fast implementation of Adaptive Histogram Equalization. Regional contrast enhancement. This function enhances a totally same result as that of the AHE1974, yet an extremely low complexity is required.
<em>Parameters</em>
 - Block Size (Width: 11/Height: 11)</p>
<p>Source: <em>Z. Wang and J. Tao, "A fast implementation of adaptive histogram equalization," in Proc. ICSP, 2006.</em></p>
<h3 id="hmclahe">HMCLAHE</h3>
<p>Histogram Modified Contrast Limited Adaptive Histogram Equalization.
<em>Parameters</em></p>
<ul>
<li>Block Size (Width: 11/Height: 11)</li>
</ul>
<p>L: 0.03
Phi: 0.50</p>
<h3 id="lce-bsescs">LCE-BSESCS</h3>
<p>Local Contrast Enhancement Utilizing Bidirectional Switching Equalization of Separated and Clipped Subhistograms.
<em>Parameters</em></p>
<ul>
<li>Block Size (Width: 33/Height: 33)</li>
</ul>
<h3 id="msrcp">MSRCP</h3>
<p>Multiscale Retinex with Chromaticity Preservation.</p>
<p>A local contrast enhancement method. It takes single channel of a color image for enhancement, thus consuming less time during process in contrast to former arts.</p>
<p><em>Parameters</em>
 - Retinex Scales
 - Small scale: 10
 - Mid Scale: 100
 - Large scale: 220</p>
<p>Source: Ana Bel´en Petro1, Catalina Sbert2, Jean-Michel Morel3 , "Multiscale Retinex," Image Processing On Line, 2014.</p>
<h3 id="noshp">NOSHP</h3>
<p>Non-Overlapped Sub-blocks and local Histogram Projection. non-overlapped sub-blocks and local histogram projection based contrast enhancement (NOSHP)</p>
<p><em>Parameters</em>
 - Block Size (Width: 127/Height: 127)</p>
<p>[Source:] <em>B. Liu, W. Jin, Y. Chen, C. Liu, and L. Li, "Contrast enhancement using non-overlapped sub-blocks and local histogram projection," TCE, vol. 57, no. 2, 2011.</em></p>
<h3 id="pohe">POHE</h3>
<p>Parametric-oriented histogram equalization (POHE)
<em>Parameters</em>
 - Block Size (Width: 127/Height: 127)</p>
<p>Source: <em>Y. F. Liu, J. M. Guo, B. S. Lai, and J. D. Lee, "High efficient contrast enhancement using parametric approximation," in Proc. IEEE ICASSP, pp. 2444-2448, 26-31 May 2013</em></p>
<h3 id="rswhe">RSWHE</h3>
<p>Recursively Separated and Weighted Histogram Equalization.
Recursively separated and weighted histogram equalization (RSWHE)</p>
<p>Source: <em>Mary Kim and Min Gyo Chung, "Recursively Separated and Weighted Histogram Equalization for Brightness Preservation and Contrast Enhancement," 2008.</em></p>
<h3 id="wallis-filter">Wallis Filter</h3>
<p>Locally adaptive contrast adjustment filter to enhance image gray-scale details. It ensures that within every specified window, the local mean and the standard deviation will match some given (user-specified) values. It is great to eliminate uneven illumination, where bright and dark tones are both present.
<em>Parameters</em></p>
<ul>
<li>Contrast: 1</li>
<li>Brightness: 0.20</li>
<li>Imposed Average: 41</li>
<li>Imposed Local StdDev: 127</li>
<li>Kernel Size: 50</li>
</ul>
<p>[Source] (https://www.microimages.com/documentation/TechGuides/55Wallis.pdf)</p>
<h2 id="feature-extraction">Feature Extraction</h2>
<h3 id="keypoint-detectors"><em>KEYPOINT DETECTORS</em></h3>
<h3 id="agast">AGAST</h3>
<p>Adaptive and Generic Accelerated Segment Test. Corner detector. It is an optimization of FAST, thus also based on Accelerated Segment Test (AST), but  its decision tree is generic, with no need to retrain the it every time.
[Source: http://www.i6.in.tum.de/Main/ResearchAgast]</p>
<p><em>Elmar Mair, Gregory D. Hager, Darius Burschka, Michael Suppa, and Gerhard Hirzinger. Adaptive and generic corner detection based on the accelerated segment test. In <em>Proceedings of the European Conference on Computer Vision (ECCV'10)</em>, September 2010</em> </p>
<h3 id="akaze">AKAZE</h3>
<p>Accelerated version of KAZE detector. In a similar fashion with KAZE, it operates in nonlinear scale space and not in Gaussian like SIFT and SURF do. Numerical methods are used to approximate the solution, namely Fast Explicit Diffusion (FED), that are proven to work much faster than any other discretization scheme.
[Source: self adjusted]</p>
<p><em>Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces. Pablo F. Alcantarilla, Jesús Nuevo and Adrien Bartoli]. In British Machine Vision Conference (BMVC), <em>Bristol, UK, September 2013</em>.</em></p>
<h3 id="brisk">BRISK</h3>
<p>A sampling pattern consisting of points lying on appropriately scaled concentric circles is applied at the neighborhood of each keypoint to retrieve gray values: processing local intensity gradients,the feature characteristic direction is determined. Finally, the oriented BRISK sampling pattern is used to obtain pairwise brightness comparison results which are assembled into the binary BRISK descriptor.</p>
<p>[Source: paper]
<em>Stefan Leutenegger, Margarita Chli, and Roland Yves Siegwart. Brisk: Binary robust invariant scalable keypoints. In <em>Computer Vision (ICCV), 2011 IEEE International Conference on</em>, pages 2548–2555. IEEE, 2011.</em></p>
<h3 id="fast">FAST</h3>
<p>Modification of the SUSAN corner detector that outperforms previously used keypoint detectors in terms of speed and reliability. Is based on Accelerated Segment Test (AST), which is used to distinguish keypoints by examining the intensity values of 16 pixels that fall in the circular pattern around the candidate pixel. A candidate pixel is considered as keypoint if there are at least N continuous pixels that have either higher or lower intensity values than it.
[Source: self adjusted]</p>
<p><em>E. Rosten. Machine Learning for High-speed Corner Detection, 2006</em></p>
<h3 id="gftt">GFTT</h3>
<p>Stands for <a href="http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf">Good features to track</a>. Modified version of the traditional Harris detector as to select only the features that pass the dissimilarity (change of appearance) test and filter out the less robust ones.
[Source: self adjusted]</p>
<p><em>Harris, C.G. and Stephens, M., 1988, August. A combined corner and edge detector. In Alvey vision conference (Vol. 15, No. 50, pp. 10-5244).</em> </p>
<h3 id="kaze">KAZE</h3>
<p>Operates in nonlinear scale space and not in Gaussian, keeping important image details <a href="http://robesafe.com/personal/pablo.alcantarilla/papers/Alcantarilla12eccv.pdf">paper</a>. The nonlinear scale space is build efficiently by means of Additive Operator Splitting (AOS) scheme.
[Source: self adjusted]</p>
<p><em>Alcantarilla, P.F., Bartoli, A. and Davison, A.J., 2012, October. KAZE features. In <em>European Conference on Computer Vision</em> (pp. 214-227). Springer, Berlin, Heidelberg.</em></p>
<h3 id="msd">MSD</h3>
<p>Maximal Self-Dissimilarity. The algorithm implements a novel interest point detector stemming from the intuition that image patches which are highly dissimilar over a relatively large extent of their surroundings hold the property of being repeatable and distinctive. This concept of "contextual self-dissimilarity" reverses the key paradigm of recent successful techniques such as the Local Self-Similarity descriptor and the Non-Local Means filter, which build upon the presence of similar - rather than dissimilar - patches. Moreover, it extends to contextual information the local self-dissimilarity notion embedded in established detectors of corner-like interest points, thereby achieving enhanced repeatability, distinctiveness and localization accuracy.
[Source: self adjusted]</p>
<p><em>Federico Tombari and Luigi Di Stefano. Interest points via maximal self-dissimilarities. In <em>Asian Conference on Computer Vision – ACCV 2014</em>, 2014.</em></p>
<h3 id="mser">MSER</h3>
<p>Maximally stable extremal region extractor.</p>
<h3 id="orb">ORB</h3>
<p>The algorithm uses FAST in pyramids to detect stable keypoints, selects the strongest features using FAST or Harris response, finds their orientation using first-order moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or k-tuples) are rotated according to the measured orientation).</p>
<p><em>Ethan Rublee, Vincent Rabaud, Kurt Konolige, Gary R. Bradski: ORB: An efficient alternative to SIFT or SURF. ICCV 2011: 2564-2571.</em></p>
<h3 id="sift">SIFT</h3>
<p>Detector part of SIFT algorithm uses Difference of Gaussians (DoG), a feature enhancement method, where image pyramids are created by repeatedly convolving the original image with Gaussian kernels. In each pyramid level, every pixel is compared with its 8 neighboring pixels in the current image as well as with its 9 neighbors of its adjacent pyramid images. Keypoints are detected as extrema in the difference between the Gaussian images.
[Source: self adjusted]</p>
<p><em>Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. <em>International journal of computer vision</em>, <em>60</em>(2), pp.91-110.</em></p>
<h3 id="star">STAR</h3>
<p>Star Feature Detector is derived from CenSurE (Center Surrounded Extrema) detector.</p>
<h3 id="surf">SURF</h3>
<p>SURF uses Fast Hessian as a detection method that is based on integral images through Hessian matrix approximation. Box type convolution filters of different sizes are used to approximate second order Gaussian derivatives for each image point. Keypoints are regions where the determinant becomes maximal through non-maximal suppression.</p>
<p><em>Bay, H., Tuytelaars, T. and Van Gool, L., 2006, May. Surf: Speeded up robust features. In <em>European conference on computer vision</em> (pp. 404-417). Springer, Berlin, Heidelberg.</em></p>
<h3 id="keypoint-descriptors"><em>KEYPOINT DESCRIPTORS</em></h3>
<h3 id="akaze_1">AKAZE</h3>
<p>A highly efficient Modified-Local Difference Binary (M-LDB) descriptor that exploits gradient and intensity information from the nonlinear scale space. The LDB descriptor follows the same principle as BRIEF, but using binary tests between the aver-age of areas instead of single pixels for additional robustness. M-LDB uses the derivatives computed in the feature detection step, reducing the number of operations required to construct the descriptor.</p>
<p><em>Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces. Pablo F. Alcantarilla, Jesús Nuevo and Adrien Bartoli]. In British Machine Vision Conference (BMVC), <em>Bristol, UK, September 2013</em>.</em></p>
<h3 id="brief">BRIEF</h3>
<p>Standing for Binary Robust Independent Elementary Features, is a short binary descriptor using the hamming distance.
Not invariant to scale and rotation.</p>
<p><em>Calonder, M., Lepetit, V., Ozuysal, M., Trzcinski, T., Strecha, C. and Fua, P., 2011. BRIEF: Computing a local binary descriptor very fast. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>34</em>(7), pp.1281-1298.</em></p>
<h3 id="brisk_1">BRISK</h3>
<h3 id="daisy">DAISY</h3>
<p>Feature descriptor that depends on histograms of gradients like SIFT but uses a Gaussian weighting and circularly symmetrical kernel.</p>
<p><em>Tola, E., Lepetit, V. and Fua, P., 2009. Daisy: An efficient dense descriptor applied to wide-baseline stereo. <em>IEEE transactions on pattern analysis and machine intelligence</em>, <em>32</em>(5), pp.815-830.</em></p>
<h3 id="freak">FREAK</h3>
<p>The algorithm propose a novel keypoint descriptor inspired by the human visual system and more precisely the retina, coined Fast Retina Key- point (FREAK). A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern. FREAKs are in general faster to compute with lower memory load and also more robust than SIFT, SURF or BRISK. They are competitive alternatives to existing keypoints in particular for embedded applications.</p>
<p><em>A. Alahi, R. Ortiz, and P. Vandergheynst. FREAK: Fast Retina Keypoint. In IEEE Conference on Computer Vision and Pattern Recognition, 2012. CVPR 2012 Open Source Award Winner.</em></p>
<h3 id="hog">HOG</h3>
<p>Histogram of Orented Gradients. Based on image gradients, histograms of gradients are calculated for pre-defiend image sub-regions. Mainly used for pedestrian detection, sensitive to rotation changes.</p>
<p><em>Rublee, E., Rabaud, V., Konolige, K. and Bradski, G.R., 2011, November. ORB: An efficient alternative to SIFT or SURF. In <em>ICCV</em> (Vol. 11, No. 1, p. 2).</em></p>
<h3 id="kaze_1">KAZE</h3>
<p><em>Alcantarilla, P.F., Bartoli, A. and Davison, A.J., 2012, October. KAZE features. In <em>European Conference on Computer Vision</em> (pp. 214-227). Springer, Berlin, Heidelberg.</em></p>
<h3 id="latch">LATCH</h3>
<p>LATCH is a binary descriptor based on learned comparisons of triplets of image patches.</p>
<p><em>Levi, G. and Hassner, T., 2016, March. LATCH: learned arrangements of three patch codes. In <em>2016 IEEE winter conference on applications of computer vision (WACV)</em> (pp. 1-9). IEEE.</em></p>
<h3 id="lucid">LUCID</h3>
<p><em>Ziegler, A., Christiansen, E., Kriegman, D. and Belongie, S.J., 2012. Locally uniform comparison image descriptor. In <em>Advances in Neural Information Processing Systems</em> (pp. 1-9).</em></p>
<h3 id="lss">LSS</h3>
<p>Local Self-Similarity is based on the texture features of images to densely calculate local self-similarity descriptors.</p>
<p><em>Shechtman E, Irani M. Matching local self-similarities across images and videos, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–8, 2007.</em></p>
<h3 id="orb_1">ORB</h3>
<p>Oriented FAST and Rotated BRIEF, as the name declares combines FAST for feature detection and BRIEF for feature description, providing invariance to rotation and robustness to noise.</p>
<p><em>Rublee, E., Rabaud, V., Konolige, K. and Bradski, G.R., 2011, November. ORB: An efficient alternative to SIFT or SURF. In <em>ICCV</em> (Vol. 11, No. 1, p. 2).</em></p>
<h3 id="sift_1">SIFT</h3>
<p>Image gradients are used  to describe the detected keypoints. Intensity values, gradient magnitude and orientation are stored, reassuring also a certain level of invariance under illumination changes.</p>
<p><em>Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. <em>International journal of computer vision</em>, <em>60</em>(2), pp.91-110.</em></p>
<h3 id="surf_1">SURF</h3>
<p>The description part of SURF describes the intensity of the neighborhood around the pixel using Haar wavelets.</p>
<p><em>Bay, H., Tuytelaars, T. and Van Gool, L., 2006, May. Surf: Speeded up robust features. In <em>European conference on computer vision</em> (pp. 404-417). Springer, Berlin, Heidelberg.</em></p>
<h2 id="feature-matching">Feature Matching</h2>
<h3 id="brute-force">Brute Force</h3>
<p>[Source: https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.html]
Every feature descriptor in the first set is matched with all other features in second set using some distance calculation, returning the closest one. <strong><em>BFMatcher.match()</em> or <em>BFMatcher.knnMatch()</em> ??</strong></p>
<p><em>NormType</em> (NORM_L1 / NORM_L2 / NORM_HAMMING / NORM_HAMMING2) --&gt; Distance measurement to be used
<em>Ratio:</em> 0.8 --&gt; ratio test proposed by D.Lowe in SIFT paper
<em>Cross Matching</em> --&gt; Cross check (bool). If it is true, matching returns only those matches with value (i,j) such that i-th descriptor in set A has j-th descriptor in set B as the best match and vice-versa. That is, the two features in both sets should match each other. It provides consistent result, and is a good alternative to ratio test proposed.
<em>Geometric Test</em>: Homography Matrix/Fundamental Matrix</p>
<pre><code>IF Homography
    Compute Method: All Points/RANSAC/LMedS/RHO
    IF All Points
        Confidence
    ELSE IF RANSAC
        Distance/Confidence/Maximum RANSAC iteration
ELSE IF LMedS
        Confidence
ELSE IF RHO
        Distance/Confidence
ELSE
    Compute Method: 7-point/8-point/RANSAC/LMedS
    IF RANSAC
            Distance/Confidence
    ELSE IF LMedS
        Confidence
    ELSE (7-point/8-point)
        Nothing
</code></pre>
<ul>
<li>Homography: Finds a perspective transformation between two planes. Can be estimated either by using all points (<code>All points</code>) to compute an initial homography estimate with a simple least-squares scheme, or RANSAC-based robust method <code>RANSAC</code>, a Least-Median robust method (<code>LMedS</code>) or the PROSAC-based robust method <code>RHO</code>. The methods <code>RANSAC</code>, <code>LMeDS</code> and <code>RHO</code> try many different random subsets of the corresponding point pairs (of four pairs each), estimate the homography matrix using this subset and a simple least-square algorithm, and then compute the quality/goodness of the computed homography (which is the number of inliers for RANSAC or the median re-projection error for LMeDs). The functions find and return the perspective transformation between the source and the destination plane so that the back-projection error is minimized. Regardless of the method, robust or not, the computed homography matrix is refined further (using inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the re-projection error even more. The method <code>RANSAC</code> and <code>RHO</code> can handle practically any ratio of outliers but it needs a threshold to distinguish inliers from outliers. The method <code>LMeDS</code> does not need any threshold but it works correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the noise is rather small, use the default method (<code>All points</code>).The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is determined up to a scale</li>
<li>Fundamental: Calculates the fundamental matrix (projective transformation with at least 7 point correspondences) from the corresponding points in two images, using either the <code>7-point algorithm</code>, the <code>8-point algorithm</code>, <code>RANSAC</code> or <code>LMedS</code>. Normally just one matrix is found. But in case of the <code>7-point algorithm</code>, the function may return up to 3 solutions.</li>
</ul>
<h3 id="flann">FLANN</h3>
<p>FLANN stands for Fast Library for Approximate Nearest Neighbors [Muja 2009]. It contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. For large datasets, it can be more efficient than Brute Force.</p>
<p><em>Ratio:</em>
<em>Cross Matching</em>
<em>Geometric Test</em>: Homography Matrix/Fundamental Matrix</p>
<pre><code>IF Homography
    Compute Method: All Points/RANSAC/LMedS/RHO
    IF All Points
        Confidence
    ELSE IF RANSAC
        Distance/Confidence/Maximum RANSAC iteration
ELSE IF LMedS
        Confidence
ELSE IF RHO
        Distance/Confidence
ELSE
    Compute Method: 7-point/8-point/RANSAC/LMedS
    IF RANSAC
            Distance/Confidence
    ELSE IF LMedS
        Confidence
    ELSE (7-point/8-point)
        Nothing
</code></pre>
<h2 id="quality-control">Quality Control</h2>
<h3 id="roc-curves">ROC Curves</h3>
<h3 id="det-curves">DET Curves</h3>
<blockquote>
<p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p>
</blockquote>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="index_old/" class="btn btn-neutral float-right" title="HELP for PhotoMatch">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
      
        <span style="margin-left: 15px"><a href="index_old/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme.js" defer></script>
      <script src="search/main.js" defer></script>

</body>
</html>

<!--
MkDocs version : 1.0.4
Build Date UTC : 2019-12-12 14:47:40
-->
