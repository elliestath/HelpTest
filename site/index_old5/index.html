<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>HELP for PhotoMatch - PhotoMatch</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>
  <link href='https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "HELP for PhotoMatch";
    var mkdocs_page_input_path = "index_old5.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> PhotoMatch</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="tocbase current">
    
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="..">HELP for PhotoMatch</a>
  </li>
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../index_old/">HELP for PhotoMatch</a>
  </li>
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../index_old2/">HELP for PhotoMatch</a>
  </li>
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../index_old3/">HELP for PhotoMatch</a>
  </li>
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../index_old4/">HELP for PhotoMatch</a>
  </li>
    
      


  
    
    <li class="navtree toctree-l1 page current">
      <a class="current" href="./">
        HELP for PhotoMatch
      </a>
    </li>
    
      



  <li class="toctree-l1 current">
    <ul class="subnav-l1 current">
    
      
          

  <li class="toctree-l2 current with-children">
    <a href="#preprocessing">
      Preprocessing
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2 current">
    <ul class="subnav-l2 current">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#decolorization">Decolorization</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#acebsf">ACEBSF</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#clahe">CLAHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#cmbfhe">CMBFHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#dhe">DHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#fahe">FAHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#hmclahe">HMCLAHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#lce-bsescs">LCE-BSESCS</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#msrcp">MSRCP</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#noshp">NOSHP</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#pohe">POHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#rswhe">RSWHE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#wallis-filter">Wallis Filter</a>
        </li>
    
    </ul>
  </li>

      
    
      
          

  <li class="toctree-l2">
    <a href="#feature-extraction">
      Feature Extraction
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2">
    <ul class="subnav-l2 toc-hidden">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#keypoint-detectors">Keypoint Detectors</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#agast">AGAST</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#akaze">AKAZE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#brisk">BRISK</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#fast">FAST</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#gftt">GFTT</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#kaze">KAZE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#msd">MSD</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#mser">MSER</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#orb">ORB</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#sift">SIFT</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#star">STAR</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#surf">SURF</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#keypoint-descriptors">Keypoint Descriptors</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#akaze_1">AKAZE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#brief">BRIEF</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#brisk_1">BRISK</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#daisy">DAISY</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#freak">FREAK</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#hog">HOG</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#kaze_1">KAZE</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#latch">LATCH</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#lucid">LUCID</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#lss">LSS</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#orb_1">ORB</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#sift_1">SIFT</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#surf_1">SURF</a>
        </li>
    
    </ul>
  </li>

      
    
      
          

  <li class="toctree-l2">
    <a href="#feature-matching">
      Feature Matching
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2">
    <ul class="subnav-l2 toc-hidden">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#brute-force">Brute Force</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#flann">FLANN</a>
        </li>
    
    </ul>
  </li>

      
    
      
          

  <li class="toctree-l2">
    <a href="#quality-control">
      Quality Control
      <span class="toctree-expand"></span>
    </a>
  </li>



  <li class="toctree-l2">
    <ul class="subnav-l2 toc-hidden">
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#roc-curves">ROC Curves</a>
        </li>
    
      
        <li class="toctree-l3">
          <a class="toctree-l4" href="#det-curves">DET Curves</a>
        </li>
    
    </ul>
  </li>

      
    
    </ul>
  </li>


  
    
  </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">PhotoMatch</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>HELP for PhotoMatch</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="help-for-photomatch">HELP for PhotoMatch</h1>
<p>Brief explanations of the algorithms used in the scientific initiative PhotoMatch. </p>
<h2 id="preprocessing">Preprocessing</h2>
<p>This step offers the option to use image enhancement techniques that could potentially facilitate the detection of robust image features.</p>
<p><strong>PARAMETERS</strong></p>
<p>Full Image Size/Max Image Size</p>
<h3 id="decolorization">Decolorization</h3>
<p>Contrast Preserving Decolorization</p>
<h3 id="acebsf">ACEBSF</h3>
<p><em>Adaptive Contrast Enhancement Based on modified Sigmoid Function.</em> </p>
<p>It first improves the poor quality images using a modified sigmoid function and then applies contrast limited adaptive histogram equalization (AHE) to enhance image contrast.</p>
<p><em>Parameters</em></p>
<ul>
<li>Blocksize (Width: 8/Height: 8)</li>
<li>L: 0.3</li>
<li>K1: 10</li>
<li>K2: 0.50</li>
</ul>
<p>[ACEBSF]: <em>Lal, S. and Chandra, M., 2014. Efficient algorithm for contrast enhancement of natural images. International  Arab Journal of Information Technology, Vol. 11(1), pp. 95-102</em></p>
<h3 id="clahe">CLAHE</h3>
<p><em>Contrast Limited Adaptive Histogram Equalization.</em></p>
<p>CLAHE is a local contrast enhancement technique robust to outliers. Local contrast enhancement methods such as adaptive histogram equalization (AHE) divide the original image into several non-overlapped sub-blocks and apply histogram equalization accordingly. CLAHE is an improvement of AHE and performs well on low contrast images.</p>
<p><em>Parameters</em></p>
<ul>
<li>Clip Limit: 40</li>
<li>Tiles Size X: 8</li>
<li>Tiles Size Y: 8</li>
</ul>
<p>[CLAHE]: <em>Zuiderveld, K., 1994. Contrast limited adaptive histogram equalization. In _Graphics gems, Vol. IV</em> pp. 474-485. Academic Press Professional, Inc._
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3092044/">Source</a> </p>
<h3 id="cmbfhe">CMBFHE</h3>
<p><em>Cascaded Multistep Binomial Filtering Histogram Equalization.</em></p>
<p>Based on the POSHE approach using cascaded multistep binomial filtering histogram equalization to reduce algorithm complexity. Similar to other local adaptive techniques, it exploits image sub-blocks (and not pixels), achieving results with effect similar to the application of a low-pass filter (LPF) avoiding the blocking effect.</p>
<p><em>Parameters</em></p>
<ul>
<li>Block Size (Width: 11/Height: 11)</li>
</ul>
<p>[CMBFHE]: <em>Lamberti, F., Montrucchio, B. and San, A., 2006. CMBFHE: a novel contrast enhancement technique based on cascaded multistep binomial filtering histogram equalization. _IEEE Transactions on Consumer Electronics</em>, Vol. <em>52</em>(3), pp.966-974._</p>
<p>[POSHE]: <em>Kim, J.Y., Kim, L.S. and Hwang, S.H., 2001. An advanced contrast enhancement using partially overlapped sub-block histogram equalization. _IEEE Transactions on Circuits and Systems for Video Technology</em>, Vol. <em>11</em>(4), pp. 475-484._</p>
<h3 id="dhe">DHE</h3>
<p><em>Dynamic Histogram Equalization.</em></p>
<p>Traditional histogram equalization employing a histogram partitioning operation to avoid dominating components and achieving better overall contrast enhancement with controlled dynamic range of gray levels while also reassuring minimum detail loss.</p>
<p><em>Parameters</em></p>
<ul>
<li>x: 1</li>
</ul>
<p>[DHE]: <em>Abdullah-Al-Wadud, M., Kabir, M.H., Dewan, M.A.A. and Chae, O., 2007. A dynamic histogram equalization for image contrast enhancement. _IEEE Transactions on Consumer Electronics</em>, Vol.  <em>53</em>(2), pp. 593-600._</p>
<h3 id="fahe">FAHE</h3>
<p><em>Fast implementation of Adaptive Histogram Equalization</em> </p>
<p>Software techniques on histogram acquisition and accumulation as well on the block size decision are used to reduce computational complexity.</p>
<p><em>Parameters</em></p>
<ul>
<li>Block Size (Width: 11/Height: 11)</li>
</ul>
<p>[FAHE]: <em>Wang, Z. and Tao, J., 2006. A fast implementation of adaptive histogram equalization. In _2006 8th international Conference on Signal Processing</em>, Vol. 2. IEEE._</p>
<h3 id="hmclahe">HMCLAHE</h3>
<p><em>Histogram Modified Contrast Limited Adaptive Histogram Equalization.</em></p>
<p>It incorporates both histogram modifications as an optimization technique and CLAHE. It controls the level of contrast enhancement, before CLAHE, resulting a strong contrast image with enhanced details.</p>
<p><em>Parameters</em></p>
<ul>
<li>Block Size (Width: 11/Height: 11)</li>
<li>L: 0.03</li>
<li>Phi: 0.50</li>
</ul>
<p>[HMCLAHE]: <em>Sundaram, M., Ramar, K., Arumugam, N. and Prabin, G., 2011. Histogram based contrast enhancement for mammogram images. In _2011 International Conference on Signal Processing, Communication, Computing and Networking Technologies</em>, pp. 842-846. IEEE._</p>
<h3 id="lce-bsescs">LCE-BSESCS</h3>
<p><em>Local Contrast Enhancement Utilizing Bidirectional Switching Equalization of Separated and Clipped Sub-histograms.</em></p>
<p><em>Parameters</em></p>
<ul>
<li>Block Size (Width: 33/Height: 33)</li>
</ul>
<h3 id="msrcp">MSRCP</h3>
<p><em>Multiscale Retinex with Chromaticity Preservation.</em></p>
<p>Based on the original Multiscale Retinex method, it applies some modifications on the post-processing steps and preserves image chromaticity. Recommended for images with a correct color distribution and white lightning.</p>
<p><em>Parameters</em></p>
<ul>
<li>Retinex Scales</li>
<li>Small scale: 10</li>
<li>Mid Scale: 100</li>
<li>Large scale: 220</li>
</ul>
<p>[MSRCP]: <em>Petro, A.B., Sbert, C. and Morel, J.M., 2014. Multiscale retinex. _Image Processing On Line</em>, pp.71-88._</p>
<h3 id="noshp">NOSHP</h3>
<p><em>Non-Overlapped Sub-blocks and local Histogram Projection.</em></p>
<p>To reduce computational complexity, it segments the image into non-overlapped sub-blocks where the histogram projection (HP) is then executed individually. Subsequently, each sub-block is related to its adjacent three ones by certain weights, so that the integral image and local details can be enhanced.</p>
<p><em>Parameters</em></p>
<ul>
<li>Block Size (Width: 127/Height: 127)</li>
</ul>
<p>[NOSHP]: <em>Liu, B., Jin, W., Chen, Y., Liu, C. and Li, L., 2011. Contrast enhancement using non-overlapped sub-blocks and local histogram projection. _IEEE Transactions on Consumer Electronics</em>, Vol. <em>57</em>(2), pp. 583-588._</p>
<h3 id="pohe">POHE</h3>
<p><em>Parametric-oriented histogram equalization (POHE)</em></p>
<p>Local enhancement method using integral images to reduce computational time during the construction of the probability distribution function and the transformation function.</p>
<p><em>Parameters</em></p>
<ul>
<li>Block Size (Width: 127/Height: 127)</li>
</ul>
<p>[POHE]: <em>Liu, Y.F., Guo, J.M., Lai, B.S. and Lee, J.D., 2013. High efficient contrast enhancement using parametric approximation. In _2013 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, pp. 2444-2448. IEEE._</p>
<h3 id="rswhe">RSWHE</h3>
<p><em>Recursively Separated and Weighted Histogram Equalization.</em></p>
<p>It segments an input histogram into two or more sub-histograms recursively, modifies them using a weighting process based on a normalized power law function, and performs HE on the weighted sub-histograms independently, achieving brightness preservation and image contrast enhancement.</p>
<p>[RSWHE]: <em>Kim, M. and Chung, M.G., 2008. Recursively separated and weighted histogram equalization for brightness preservation and contrast enhancement. _IEEE Transactions on Consumer Electronics</em>, Vol., <em>54</em>(3), pp. 1389-1397._</p>
<h3 id="wallis-filter">Wallis Filter</h3>
<p>Locally adaptive contrast adjustment filter to enhance image gray-scale details. It ensures that within every specified window, the local mean and the standard deviation will match some given (user-specified) values. Used to eliminate uneven illumination, where bright and dark tones are both present.</p>
<p><em>Parameters</em></p>
<ul>
<li>Contrast: 1</li>
<li>Brightness: 0.20</li>
<li>Imposed Average: 41</li>
<li>Imposed Local StdDev: 127</li>
<li>Kernel Size: 50</li>
</ul>
<p>[Source] (https://www.microimages.com/documentation/TechGuides/55Wallis.pdf)</p>
<h2 id="feature-extraction">Feature Extraction</h2>
<h3 id="keypoint-detectors"><em>Keypoint Detectors</em></h3>
<h3 id="agast">AGAST</h3>
<p>Adaptive and Generic Accelerated Segment Test. Corner detector. It is an optimization of FAST, thus also based on Accelerated Segment Test (AST), but  its decision tree is generic, with no need to retrain the it every time.</p>
<p>[Source: http://www.i6.in.tum.de/Main/ResearchAgast]</p>
<p>[AGAST]: <em>Mair, E., Hager, G.D., Burschka, D., Suppa, M. and Hirzinger, G., 2010. Adaptive and generic corner detection based on the accelerated segment test. In _European conference on Computer vision</em>, pp. 183-196. Springer, Berlin, Heidelberg._</p>
<h3 id="akaze">AKAZE</h3>
<p>Accelerated version of KAZE detector. In a similar fashion with KAZE, it operates in nonlinear scale space and not in Gaussian like SIFT and SURF do. Numerical methods are used to approximate the solution, namely Fast Explicit Diffusion (FED), that are proven to work much faster than any other discretization scheme.
[Source: self adjusted]</p>
<p>[AKAZE]: <em>Alcantarilla, P.F., Nuevo, J., Bartoli, A., 2013. Fast explicit diffusion for accelerated features in nonlinear scale spaces. In Proc. BMVC, Vol. 34(7), pp. 1281–1298.</em></p>
<h3 id="brisk">BRISK</h3>
<p>A sampling pattern consisting of points lying on appropriately scaled concentric circles is applied at the neighborhood of each keypoint to retrieve gray values: processing local intensity gradients,the feature characteristic direction is determined. Finally, the oriented BRISK sampling pattern is used to obtain pairwise brightness comparison results which are assembled into the binary BRISK descriptor.</p>
<p>[Source: paper]</p>
<p>[BRISK]: <em>Leutenegger, S., Chli, M. and Siegwart, R., 2011. BRISK: Binary robust invariant scalable keypoints. In _2011 IEEE International Conference on Computer Vision (ICCV)</em>, pp. 2548-2555. IEEE._</p>
<h3 id="fast">FAST</h3>
<p>Modification of the SUSAN corner detector that outperforms previously used keypoint detectors in terms of speed and reliability. Is based on Accelerated Segment Test (AST), which is used to distinguish keypoints by examining the intensity values of 16 pixels that fall in the circular pattern around the candidate pixel. A candidate pixel is considered as keypoint if there are at least N continuous pixels that have either higher or lower intensity values than it.
[Source: self adjusted]</p>
<p>[FAST]: <em>Rosten, E. and Drummond, T., 2006. Machine learning for high-speed corner detection. In _European conference on computer vision</em>, pp. 430-443. Springer, Berlin, Heidelberg._</p>
<h3 id="gftt">GFTT</h3>
<p>Stands for <a href="http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf">Good features to track</a>. Modified version of the traditional Harris detector as to select only the features that pass the dissimilarity (change of appearance) test and filter out the less robust ones.</p>
<p>[Source: self adjusted]</p>
<p>[GFTT]: <em>Harris, C.G. and Stephens, M., 1988. A combined corner and edge detector. In _Alvey vision conference</em>, Vol. 15(50), pp. 10-5244._</p>
<h3 id="kaze">KAZE</h3>
<p>Operates in nonlinear scale space and not in Gaussian, keeping important image details <a href="http://robesafe.com/personal/pablo.alcantarilla/papers/Alcantarilla12eccv.pdf">paper</a>. The nonlinear scale space is build efficiently by means of Additive Operator Splitting (AOS) scheme.</p>
<p>[Source: self adjusted]</p>
<p>[KAZE]: <em>Alcantarilla, P.F., Bartoli, A. and Davison, A.J., 2012. KAZE features. In _European Conference on Computer Vision</em>, pp. 214-227. Springer, Berlin, Heidelberg._</p>
<h3 id="msd">MSD</h3>
<p>Maximal Self-Dissimilarity. The algorithm implements a novel interest point detector stemming from the intuition that image patches which are highly dissimilar over a relatively large extent of their surroundings hold the property of being repeatable and distinctive. This concept of "contextual self-dissimilarity" reverses the key paradigm of recent successful techniques such as the Local Self-Similarity descriptor and the Non-Local Means filter, which build upon the presence of similar - rather than dissimilar - patches. Moreover, it extends to contextual information the local self-dissimilarity notion embedded in established detectors of corner-like interest points, thereby achieving enhanced repeatability, distinctiveness and localization accuracy.</p>
<p>[Source: self adjusted]</p>
<p>[MSD]: <em>Tombari, F. and Di Stefano, L., 2014. Interest points via maximal self-dissimilarities. In _Asian Conference on Computer Vision</em>, pp. 586-600. Springer, Cham._</p>
<h3 id="mser">MSER</h3>
<p>Maximally stable extremal region extractor.</p>
<h3 id="orb">ORB</h3>
<p>The algorithm uses FAST in pyramids to detect stable keypoints, selects the strongest features using FAST or Harris response, finds their orientation using first-order moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or k-tuples) are rotated according to the measured orientation).</p>
<p>[ORB]: <em>Rublee, E., Rabaud, V., Konolige, K. and Bradski, G.R., 2011. ORB: An efficient alternative to SIFT or SURF. In _ICCV</em>, Vol. 11(1), pp. 2._</p>
<h3 id="sift">SIFT</h3>
<p>Detector part of SIFT algorithm uses Difference of Gaussians (DoG), a feature enhancement method, where image pyramids are created by repeatedly convolving the original image with Gaussian kernels. In each pyramid level, every pixel is compared with its 8 neighboring pixels in the current image as well as with its 9 neighbors of its adjacent pyramid images. Keypoints are detected as extrema in the difference between the Gaussian images.</p>
<p>[Source: self adjusted]</p>
<p>[SIFT]: <em>Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. _International journal of computer vision</em>, Vol. 60(2), pp.91-110._</p>
<h3 id="star">STAR</h3>
<p>Star Feature Detector is derived from CenSurE (Center Surrounded Extrema) detector.</p>
<h3 id="surf">SURF</h3>
<p>SURF uses Fast Hessian as a detection method that is based on integral images through Hessian matrix approximation. Box type convolution filters of different sizes are used to approximate second order Gaussian derivatives for each image point. Keypoints are regions where the determinant becomes maximal through non-maximal suppression.</p>
<p>[SURF]: <em>Bay, H., Tuytelaars, T. and Van Gool, L., 2006. Surf: Speeded up robust features. In _European conference on computer vision</em>, pp. 404-417. Springer, Berlin, Heidelberg._</p>
<h3 id="keypoint-descriptors"><em>Keypoint Descriptors</em></h3>
<h3 id="akaze_1">AKAZE</h3>
<p>A highly efficient Modified-Local Difference Binary (M-LDB) descriptor that exploits gradient and intensity information from the nonlinear scale space. The LDB descriptor follows the same principle as BRIEF, but using binary tests between the aver-age of areas instead of single pixels for additional robustness. M-LDB uses the derivatives computed in the feature detection step, reducing the number of operations required to construct the descriptor.</p>
<p>[AKAZE]: <em>Alcantarilla, P.F., Nuevo, J., Bartoli, A., 2013. Fast explicit diffusion for accelerated features in nonlinear scale spaces. In Proc. BMVC, Vol. 34(7), pp. 1281–1298.</em></p>
<h3 id="brief">BRIEF</h3>
<p>Standing for Binary Robust Independent Elementary Features, is a short binary descriptor using the hamming distance.
Not invariant to scale and rotation.</p>
<p>[BRIEF]: <em>Calonder, M., Lepetit, V., Ozuysal, M., Trzcinski, T., Strecha, C. and Fua, P., 2011. BRIEF: Computing a local binary descriptor very fast. _IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, Vol. 34(7), pp.1281-1298._</p>
<h3 id="brisk_1">BRISK</h3>
<h3 id="daisy">DAISY</h3>
<p>Feature descriptor that depends on histograms of gradients like SIFT but uses a Gaussian weighting and circularly symmetrical kernel.</p>
<p>[DAISY]: <em>Tola, E., Lepetit, V. and Fua, P., 2009. Daisy: An efficient dense descriptor applied to wide-baseline stereo. _IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, Vol. 32(5), pp. 815-830._</p>
<h3 id="freak">FREAK</h3>
<p>The algorithm propose a novel keypoint descriptor inspired by the human visual system and more precisely the retina, coined Fast Retina Key- point (FREAK). A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern. FREAKs are in general faster to compute with lower memory load and also more robust than SIFT, SURF or BRISK. They are competitive alternatives to existing keypoints in particular for embedded applications.</p>
<p>[FREAK]: <em>Alahi, A., Ortiz, R. and Vandergheynst, P.. Freak: Fast retina keypoint. In _2012 IEEE Conference on Computer Vision and Pattern Recognition</em>, pp. 510-517. IEEE._</p>
<h3 id="hog">HOG</h3>
<p>Histogram of Orented Gradients. Based on image gradients, histograms of gradients are calculated for pre-defiend image sub-regions. Mainly used for pedestrian detection, sensitive to rotation changes.</p>
<h3 id="kaze_1">KAZE</h3>
<p>[KAZE]: <em>Alcantarilla, P.F., Bartoli, A. and Davison, A.J., 2012. KAZE features. In _European Conference on Computer Vision</em>, pp. 214-227. Springer, Berlin, Heidelberg._</p>
<h3 id="latch">LATCH</h3>
<p>LATCH is a binary descriptor based on learned comparisons of triplets of image patches.</p>
<p>[LATCH]: <em>Levi, G. and Hassner, T., 2016. LATCH: learned arrangements of three patch codes. In _2016 IEEE winter conference on applications of computer vision (WACV)</em>, pp. 1-9. IEEE._</p>
<h3 id="lucid">LUCID</h3>
<p>[LUCID]: <em>Ziegler, A., Christiansen, E., Kriegman, D. and Belongie, S.J., 2012. Locally uniform comparison image descriptor. In _Advances in Neural Information Processing Systems</em>, pp. 1-9._</p>
<h3 id="lss">LSS</h3>
<p>Local Self-Similarity is based on the texture features of images to densely calculate local self-similarity descriptors.</p>
<p>[LSS]: <em>Shechtman, E. and Irani, M., 2007. Matching Local Self-Similarities across Images and Videos. In _CVPR</em>, Vol. 2, pp. 3._</p>
<h3 id="orb_1">ORB</h3>
<p>Oriented FAST and Rotated BRIEF, as the name declares combines FAST for feature detection and BRIEF for feature description, providing invariance to rotation and robustness to noise.</p>
<p>[ORB]: <em>Rublee, E., Rabaud, V., Konolige, K. and Bradski, G.R., 2011. ORB: An efficient alternative to SIFT or SURF. In _ICCV</em>, Vol. 11(1), pp. 2._</p>
<h3 id="sift_1">SIFT</h3>
<p>Image gradients are used  to describe the detected keypoints. Intensity values, gradient magnitude and orientation are stored, reassuring also a certain level of invariance under illumination changes.</p>
<p>[SIFT]: <em>Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. _International journal of computer vision</em>, Vol. 60(2), pp.91-110._</p>
<h3 id="surf_1">SURF</h3>
<p>The description part of SURF describes the intensity of the neighborhood around the pixel using Haar wavelets.</p>
<p>[SURF]: <em>Bay, H., Tuytelaars, T. and Van Gool, L., 2006. Surf: Speeded up robust features. In _European conference on computer vision</em>, pp. 404-417. Springer, Berlin, Heidelberg._</p>
<h2 id="feature-matching">Feature Matching</h2>
<h3 id="brute-force">Brute Force</h3>
<p>[Source: https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_matcher/py_matcher.html]</p>
<p>Every feature descriptor in the first set is matched with all other features in second set using some distance calculation, returning the closest one. <strong><em>BFMatcher.match()</em> or <em>BFMatcher.knnMatch()</em> ??</strong></p>
<p><em>Parameters</em></p>
<ul>
<li><em>NormType</em></li>
</ul>
<p>NORM_L1 / NORM_L2 / NORM_HAMMING / NORM_HAMMING2 --&gt; Distance measurement to be used</p>
<ul>
<li><em>Ratio:</em></li>
</ul>
<p>0.8: ratio test proposed by D.Lowe in SIFT paper</p>
<ul>
<li><em>Cross Matching</em></li>
</ul>
<p>Cross check (bool). If it is true, matching returns only those matches with value (i,j) such that i-th descriptor in set A has j-th descriptor in set B as the best match and vice-versa. That is, the two features in both sets should match each other. It provides consistent result, and is a good alternative to ratio test proposed.</p>
<ul>
<li><em>Geometric Test</em></li>
</ul>
<p>Homography Matrix/Fundamental Matrix</p>
<pre><code>IF Homography
    Compute Method: All Points/RANSAC/LMedS/RHO
    IF All Points
        Confidence
    ELSE IF RANSAC
        Distance/Confidence/Maximum RANSAC iteration
ELSE IF LMedS
        Confidence
ELSE IF RHO
        Distance/Confidence
ELSE
    Compute Method: 7-point/8-point/RANSAC/LMedS
    IF RANSAC
            Distance/Confidence
    ELSE IF LMedS
        Confidence
    ELSE (7-point/8-point)
        Nothing
</code></pre>
<ol>
<li>Homography</li>
</ol>
<p>Finds a perspective transformation between two planes. Can be estimated either by using all points (<code>All points</code>) to compute an initial homography estimate with a simple least-squares scheme, or RANSAC-based robust method <code>RANSAC</code>, a Least-Median robust method (<code>LMedS</code>) or the PROSAC-based robust method <code>RHO</code>. The methods <code>RANSAC</code>, <code>LMeDS</code> and <code>RHO</code> try many different random subsets of the corresponding point pairs (of four pairs each), estimate the homography matrix using this subset and a simple least-square algorithm, and then compute the quality/goodness of the computed homography (which is the number of inliers for RANSAC or the median re-projection error for LMeDs). The functions find and return the perspective transformation between the source and the destination plane so that the back-projection error is minimized. Regardless of the method, robust or not, the computed homography matrix is refined further (using inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the re-projection error even more. The method <code>RANSAC</code> and <code>RHO</code> can handle practically any ratio of outliers but it needs a threshold to distinguish inliers from outliers. The method <code>LMeDS</code> does not need any threshold but it works correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the noise is rather small, use the default method (<code>All points</code>).The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is determined up to a scale.</p>
<ol>
<li>Fundamental</li>
</ol>
<p>Calculates the fundamental matrix (projective transformation with at least 7 point correspondences) from the corresponding points in two images, using either the <code>7-point algorithm</code>, the <code>8-point algorithm</code>, <code>RANSAC</code> or <code>LMedS</code>. Normally just one matrix is found. But in case of the <code>7-point algorithm</code>, the function may return up to 3 solutions.</p>
<h3 id="flann">FLANN</h3>
<p>FLANN stands for Fast Library for Approximate Nearest Neighbors [Muja 2009]. It contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. For large datasets, it can be more efficient than Brute Force.</p>
<ul>
<li><em>Ratio:</em></li>
<li><em>Cross Matching</em></li>
<li><em>Geometric Test</em></li>
</ul>
<p>Homography Matrix/Fundamental Matrix</p>
<pre><code>IF Homography
    Compute Method: All Points/RANSAC/LMedS/RHO
    IF All Points
        Confidence
    ELSE IF RANSAC
        Distance/Confidence/Maximum RANSAC iteration
ELSE IF LMedS
        Confidence
ELSE IF RHO
        Distance/Confidence
ELSE
    Compute Method: 7-point/8-point/RANSAC/LMedS
    IF RANSAC
            Distance/Confidence
    ELSE IF LMedS
        Confidence
    ELSE (7-point/8-point)
        Nothing
</code></pre>
<h2 id="quality-control">Quality Control</h2>
<h3 id="roc-curves">ROC Curves</h3>
<h3 id="det-curves">DET Curves</h3>
<blockquote>
<p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p>
</blockquote>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../index_old4/" class="btn btn-neutral" title="HELP for PhotoMatch"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../index_old4/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
