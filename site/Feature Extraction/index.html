<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Feature Extraction - PhotoMatch</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>
  <link href='https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Feature Extraction";
    var mkdocs_page_input_path = "Feature Extraction.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> PhotoMatch</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="tocbase current">
    
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="..">PhotoMatch</a>
  </li>
    
      


  
    
    <li class="navtree toctree-l1 page current">
      <a class="current" href="./">
        Feature Extraction
      </a>
    </li>
    
      



  <li class="toctree-l1 current">
    <ul class="subnav-l1 current">
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#keypoint-detectors">Keypoint Detectors</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#agast">AGAST</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#akaze">AKAZE</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#brisk">BRISK</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#fast">FAST</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#gftt">GFTT</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#kaze">KAZE</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#msd">MSD</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#mser">MSER</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#orb">ORB</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#sift">SIFT</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#star">STAR</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#surf">SURF</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#keypoint-descriptors">Keypoint Descriptors</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#akaze_1">AKAZE</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#boost">BOOST</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#brief">BRIEF</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#brisk_1">BRISK</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#daisy">DAISY</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#freak">FREAK</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#hog">HOG</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#kaze_1">KAZE</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#latch">LATCH</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#lss">LSS</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#orb_1">ORB</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#vgg">VGG</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#sift_1">SIFT</a>
        </li>
    
      
        <li class="toctree-l2">
          <a class="toctree-l3" href="#surf_1">SURF</a>
        </li>
    
    </ul>
  </li>


  
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../Feature Matching/">Feature Matching</a>
  </li>
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../Image Preprocessing/">Preprocessing</a>
  </li>
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../Quality Control/">Quality Control</a>
  </li>
    
  </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">PhotoMatch</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Feature Extraction</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="feature-extraction">Feature Extraction</h1>
<h2 id="keypoint-detectors"><em>Keypoint Detectors</em></h2>
<h2 id="agast">AGAST</h2>
<p><em>Adaptive and Generic Accelerated Segment Test.</em> </p>
<p>Corner detector introduced as an optimization of FAST, achieving higher computational efficiency.  AGAST is also based on Accelerated Segment Test (AST), but  its decision tree is generic with no need to retrain, as it adapts according to the current region of the images being processed.</p>
<p>[Source: self adjusted] + <a href="http://www.i6.in.tum.de/Main/ResearchAgast">Source</a></p>
<p>[<a href="https://mediatum.ub.tum.de/doc/1287456/file.pdf">AGAST</a>]: <em>Mair, E., Hager, G.D., Burschka, D., Suppa, M. and Hirzinger, G., 2010. Adaptive and generic corner detection based on the accelerated segment test. In European conference on Computer vision, pp. 183-196. Springer, Berlin, Heidelberg.</em></p>
<h2 id="akaze">AKAZE</h2>
<p><em>Accelerated KAZE</em></p>
<p>In a similar fashion with KAZE,  this blob detector operates in nonlinear scale space and not in Gaussian like SIFT and SURF. Numerical methods are used to approximate the solution, namely Fast Explicit Diffusion (FED), that are proven to work much faster than any other discretization scheme. The detector is based on the determinant of the Hessian Matrix, while Scharr filters are used to provide rotational invariance.</p>
<p>[Source: self adjusted]</p>
<p>[<a href="http://www.bmva.org/bmvc/2013/Papers/paper0013/abstract0013.pdf">AKAZE</a>]: <em>Alcantarilla, P.F., Nuevo, J., Bartoli, A., 2013. Fast explicit diffusion for accelerated features in nonlinear scale spaces. In Proc. BMVC, Vol. 34(7), pp. 1281â€“1298.</em></p>
<h2 id="brisk">BRISK</h2>
<p><em>Binary Robust Invariant Scalable Keypoints</em></p>
<p>Corner detector robust to scale and rotation changes. Using AGAST detection methods, a sampling pattern consisting of points lying on appropriately scaled concentric circles is applied at the neighborhood of each keypoint to retrieve gray values: processing local intensity gradients,the feature characteristic direction is determined. </p>
<p>[Source: paper]+ [Source:self-adjusted]</p>
<p>[<a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/43288/1/eth-7684-01.pdf">BRISK</a>]: <em>Leutenegger, S., Chli, M. and Siegwart, R., 2011. BRISK: Binary robust invariant scalable keypoints. In 2011 IEEE International Conference on Computer Vision a(ICCV), pp. 2548-2555. IEEE.</em></p>
<h2 id="fast">FAST</h2>
<p><em>Features from Accelerated Segment Test</em></p>
<p>Modification of the SUSAN corner detector that outperforms previously used detectors in terms of speed and reliability. Is based on Accelerated Segment Test (AST), which is used to distinguish keypoints by examining the intensity values of 16 pixels in a circular pattern around the candidate keypoint pixel. A candidate pixel is considered as keypoint based on a segment test, i.e. if there are at least N continuous pixels that have either higher or lower intensity values than it. Because of his effectiveness, it is suitable for real-time applications.</p>
<p>[Source: self adjusted]</p>
<p>[<a href="https://link.springer.com/chapter/10.1007/11744023_34">FAST</a>]: <em>Rosten, E. and Drummond, T., 2006. Machine learning for high-speed corner detection. In European conference on computer vision, pp. 430-443. Springer, Berlin, Heidelberg.</em></p>
<h2 id="gftt">GFTT</h2>
<p><em>Good Features to Track.</em> </p>
<p>Modified version of the traditional Harris detector as to select only the features that have large eignevalues and pass the dissimilarity  test. Thus, it filters out the less robust ones through a temporal monitoring. In more detail, in order to keep the more robust features, it then rejects corners with minimum eigenvalues less than a threshold. Finally, it further rejects weak corners that are closer than a pre-defined distance to a strong corner. Invariant to affine changes.</p>
<p>[Source: self adjusted]</p>
<p><a href="http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf">Good features to track</a></p>
<p>[Harris]: <em>Harris, C.G. and Stephens, M., 1988. A combined corner and edge detector. In Alvey vision conference, Vol. 15(50), pp. 10-5244.</em></p>
<p>[GFTT]: <em>J. Shi and C. Tomasi. Good features to track. In IEEE Computer Society Conference on Computer Vi-sion and Pattern Recognition, pages 593â€“600, 1994.</em></p>
<h2 id="kaze">KAZE</h2>
<p>Blob detector that operates in nonlinear scale space through non-linear diffusion filtering (and not in Gaussian), keeping important image details. KAZE detector is based on scale normalized determinant of Hessian Matrix, computed at multiple non-linear scale levels. </p>
<p>[Source: self adjusted]</p>
<p>[<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.304.4980&amp;rep=rep1&amp;type=pdf">KAZE</a>]: <em>Alcantarilla, P.F., Bartoli, A. and Davison, A.J., 2012. KAZE features. In European Conference on Computer Vision, pp. 214-227. Springer, Berlin, Heidelberg.</em></p>
<h2 id="msd">MSD</h2>
<p><em>Maximal Self-Dissimilarity.</em> </p>
<p>The algorithm implements a detector based on the assumption that image patches (edges, corners, blobs etc) which are highly dissimilar over a relatively large extent of their surroundings tend to be repeatable and distinctive. It relies on the computation of a patchâ€™s self-similarity over an extended neighborhood and more precisely it  determines whether or not a pixel (keypoint) shows similar patches in its surroundings using Contextual Self-Dissimilarity (CSD). It provided invariance to viewpoint variations and illmination changes.</p>
<p>[Source: self adjusted]</p>
<p>[<a href="http://imagine.enpc.fr/~monasse/Stereo/Projects/TombariDiStefano14.pdf">MSD</a>]: <em>Tombari, F. and Di Stefano, L., 2014. Interest points via maximal self-dissimilarities. In Asian Conference on Computer Vision, pp. 586-600. Springer, Cham.</em></p>
<h2 id="mser">MSER</h2>
<p><em>Maximally Stable Extremal Region Extractor.</em></p>
<p>Blob detection algorithm of nearly linear complexity, invariant to scale, rotation and affine transformations tackling wide baseline matching. First, distinguished regions are detected and multiple scaled measurement regions are associated with them. MSER has been sucessfully used for tracking applications.</p>
<p>[Source:self-adjusted]</p>
<p><a href="https://pdfs.semanticscholar.org/e045/aafb44693c9de943c750a6b60f2153ac3ccb.pdf">MSER</a>: <em>Matas, J., Chum, O., Urban, M. and Pajdla, T., 2004. Robust wide-baseline stereo from maximally stable extremal regions. _Image and vision computing</em>, <em>22</em>(10), pp.761-767._</p>
<h2 id="orb">ORB</h2>
<p><em>Oriented FAST and Rotated BRIEF</em></p>
<p>Corner detection algorithm that uses modified FAST (oFAST) in pyramids to detect stable keypoints over scale changes. It selects the strongest corners using FAST or Harris response (Harris Corner score) and finds their orientation using first-order moments.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.370.4395&amp;rep=rep1&amp;type=pdf">ORB</a>]: <em>Rublee, E., Rabaud, V., Konolige, K. and Bradski, G.R., 2011. ORB: An efficient alternative to SIFT or SURF. In ICCV, Vol. 11(1), pp. 2.</em></p>
<h2 id="sift">SIFT</h2>
<p><em>Scale Invariant Feature Transform</em></p>
<p>Blob detector using Difference of Gaussians (DoG), a feature enhancement method, where image pyramids are created by repeatedly convolving the original image with Gaussian kernels. In each pyramid level, every pixel is compared with its 8 neighboring pixels in the current image as well as with its 9 neighbors of its adjacent pyramid images. Keypoints are detected as extrema (local maxima) in the difference between the Gaussian images.</p>
<p>[Source: self adjusted]</p>
<p>[<a href="https://link.springer.com/article/10.1023/B:VISI.0000029664.99615.94">SIFT</a>]: <em>Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. International journal of computer vision, Vol. 60(2), pp.91-110.</em></p>
<h2 id="star">STAR</h2>
<p>Star Feature Detector derived from CenSurE (Center Surrounded Extrema) detector. STAR is multiscale and works on full spatial resolution, using a bi-level approximation of the Laplacian of Gaussians (LoG) filter and integral images for computational efficiency. Scale space is generated by using masks of different sizes.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.1117&amp;rep=rep1&amp;type=pdf">CENSURE</a>]: <em>Agrawal, M., Konolige, K. and Blas, M.R., 2008. Censure: Center surround extremas for realtime feature detection and matching. In European Conference on Computer Vision, pp. 102-115. Springer, Berlin, Heidelberg.</em></p>
<h2 id="surf">SURF</h2>
<p><em>Speeded-Up Robust Features</em></p>
<p>SURF blob detector uses Fast Hessian (Hessian-Laplace) as a detection method that is based on integral images through Hessian matrix approximation. Box type convolution filters of different sizes are used to approximate second order Gaussian derivatives for each image point. Keypoints are regions where the determinant becomes maximal through non-maximal suppression. </p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://link.springer.com/chapter/10.1007/11744023_32">SURF</a>]: <em>Bay, H., Tuytelaars, T. and Van Gool, L., 2006. Surf: Speeded up robust features. In European conference on computer vision, pp. 404-417. Springer, Berlin, Heidelberg.</em></p>
<h2 id="keypoint-descriptors"><em>Keypoint Descriptors</em></h2>
<h2 id="akaze_1">AKAZE</h2>
<p><em>Accelerated KAZE</em></p>
<p>A highly efficient Modified Local Difference Binary (MLDB) descriptor that uses gradient and intensity information from the nonlinear scale space. The LDB descriptor is similar to BRIEF, but using binary tests between the average of areas instead of single pixels for additional robustness. MLDB uses the derivatives computed in the feature detection step.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="http://www.bmva.org/bmvc/2013/Papers/paper0013/abstract0013.pdf">AKAZE</a>]: <em>Alcantarilla, P.F., Nuevo, J., Bartoli, A., 2013. Fast explicit diffusion for accelerated features in nonlinear scale spaces. In Proc. BMVC, Vol. 34(7), pp. 1281â€“1298.</em></p>
<h2 id="boost">BOOST</h2>
<p><em>Learning Image Descriptors with Boosting</em></p>
<p>Supervised learning framework detecting low dimensional but highly discriminative features. It applies boosting to learn a set of binary hash functions that achieve a performance comparable to real-valuedother descriptors such as BRIEF and BRISK, by optimizing their sampling patterns. Each bit is computed as a thresholded linear combination of a set of weak learners.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="Trzcinski_Boosting_Binary_Keypoint_2013_CVPR_paper">BOOST</a>]: <em>Trzcinski, T., Christoudias, M., Fua, P. and Lepetit, V., 2013. Boosting binary keypoint descriptors. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 2874-2881).</em></p>
<h2 id="brief">BRIEF</h2>
<p><em>Binary Robust Independent Elementary Features</em></p>
<p>A short binary descriptor inspired by the census transform extracting bit strings (intensities) which are therefore compared along the same lines. Simple binary tests are used on smoothed image patches. Using pre-trained binary tests on classificatiion trees, the signature od any arbitrary keypoint can be obtained. Although originally not invariant to scale and rotation, several variations existi such as the upright (U-BRIEF), oriented (O-BRIEF) or scaled (S-BRIEF).</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://ieeexplore.ieee.org/abstract/document/6081878">BRIEF</a>]: <em>Calonder, M., Lepetit, V., Ozuysal, M., Trzcinski, T., Strecha, C. and Fua, P., 2011. BRIEF: Computing a local binary descriptor very fast. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 34(7), pp.1281-1298.</em></p>
<h2 id="brisk_1">BRISK</h2>
<p><em>Binary Robust Invariant Scalable Keypoints</em></p>
<p>It uses a pattern of points circually distributed around each detected feature, estimates the dominant orientation by local gradient in order to achieve rotation invariance. BRISK features are invariant to scale, rotation, and limited affine changes.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/43288/1/eth-7684-01.pdf">BRISK</a>]: <em>Leutenegger, S., Chli, M. and Siegwart, R., 2011. BRISK: Binary robust invariant scalable keypoints. In 2011 IEEE International Conference on Computer Vision (ICCV), pp. 2548-2555. IEEE.</em></p>
<h2 id="daisy">DAISY</h2>
<p>Feature descriptor that  retains the robustness of other descriptors like SIFT or GLOH, yet can be efficiently computed densly for every pixel generating depth maps. Designed to work also with wide-baseline scenarios, it depends on histograms of gradients like SIFT but uses a Gaussian weighting, i.e. sum of convolutions to achieve speed, and circularly symmetrical kernel. Robust to scale, viewpoint, brightness and image quality transformations.</p>
<p>[<a href="https://ieeexplore.ieee.org/abstract/document/4815264">DAISY</a>]: <em>Tola, E., Lepetit, V. and Fua, P., 2009. Daisy: An efficient dense descriptor applied to wide-baseline stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 32(5), pp. 815-830.</em></p>
<h2 id="freak">FREAK</h2>
<p><em>Fast Retina Keypoint</em></p>
<p>The algorithm propose a novel keypoint descriptor inspired by the human visual system and and its retina. A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern, that, similarly to BRIEF is circular but with a higher density closer to the keypoint.  They are competitive alternatives to existing keypoints in particular for embedded applications.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://ieeexplore.ieee.org/abstract/document/6247715">FREAK</a>]: <em>Alahi, A., Ortiz, R. and Vandergheynst, P., 2012. Freak: Fast retina keypoint. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 510-517. IEEE.</em></p>
<h2 id="hog">HOG</h2>
<p><em>Histogram of Oriented Gradients</em></p>
<p>Based on image gradients, histograms of gradients (HOG) are calculated for pre-defiend image subregions. For each feature point, a square block is defined.  Applying the Sobel operator, gradient orientation and magnitutes are computed. Pixels are assigned to orientation bins and for each cell a vector of nine values is obtained. Mainly used for pedestrian detection and tracking, sensitive to rotation changes.</p>
<p>[<a href="https://hal.inria.fr/file/index/docid/548512/filename/hog_cvpr2005.pdf">HOG</a>]: <em>Dalal, N. and Triggs, B., 2005. Histograms of oriented gradients for human detection. In IEEE International Conference on Computer Vision and Pattern Recognition, V. 2, pp. 886â€“893.</em></p>
<h2 id="kaze_1">KAZE</h2>
<p>KAZE descriptor reassures rotation invariance by finding the dominant orientation within a circular region around every detected feature using a variant of the SURF descriptor. KAZE features are invariant to rotation, scale, limited affine and have more distinctiveness at varying scales with the cost of moderate increase in computational time.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.304.4980&amp;rep=rep1&amp;type=pdf">KAZE</a>]: <em>Alcantarilla, P.F., Bartoli, A. and Davison, A.J., 2012. KAZE features. In European Conference on Computer Vision, pp. 214-227. Springer, Berlin, Heidelberg.</em></p>
<h2 id="latch">LATCH</h2>
<p><em>Learned Arrangements of Three Patch Codes</em></p>
<p>LATCH is a binary descriptor based on learned comparisons of triplets of image patches instead of pairs to improve robustness. Patch triplets are introduced in order to improve robustness</p>
<p>[<a href="https://arxiv.org/pdf/1501.03719.pdf">LATCH</a>]: <em>Levi, G. and Hassner, T., 2016. LATCH: learned arrangements of three patch codes. In 2016 IEEE winter conference on applications of computer vision (WACV), pp. 1-9. IEEE.</em></p>
<h2 id="lss">LSS</h2>
<p><em>Local Self-Similarity</em></p>
<p>Local Self-Similarity is based on the texture features of images to densely calculate local self-similarity descriptors on patch-level. Self similarities are calculated locally within image regions centred around certain patches using sum of squared sum metrics (SSD).  Applicable also for registering video sequences and image retrieval applications as well as object detection and classification.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="http://image.ntua.gr/iva/files/ShechtmanIrani_CVPR2007%20-%20Matching%20Local%20Self-Similarities%20Across%20Images%20and%20Videos.pdf">LSS</a>]: <em>Shechtman, E. and Irani, M., 2007. Matching Local Self-Similarities across Images and Videos. In CVPR, Vol. 2, pp. 3.</em></p>
<h2 id="orb_1">ORB</h2>
<p><em>Oriented FAST and Rotated BRIEF</em></p>
<p>The descriptor part of ORB is developed in a similar fashion to BRIEF, yet it also provides additional invariance to rotation and robustness to noise. A rotation-aware version of BRIEF (rBRIEF) is used and then a learning method is applied to choose a good subset of binary tests, identifiying features that have high variance and are uncorrelated.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.370.4395&amp;rep=rep1&amp;type=pdf">ORB</a>]: <em>Rublee, E., Rabaud, V., Konolige, K. and Bradski, G.R., 2011. ORB: An efficient alternative to SIFT or SURF. In ICCV, Vol. 11(1), pp. 2.</em></p>
<h2 id="vgg">VGG</h2>
<p>An Oxford Visual Geometry Group descriptor trained end to end using "Descriptor Learning Using Convex Optimisation" (DLCO). Indeed, it tackles the problem of learning pooling regions as a convex optimization, i.e. selecting a few regions among a large set of candidate ones. Dimensionality reduction and discrimination of the the descriptor are also solved by learning. Comparable with other state of the art real value and binary descriptors.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://ieeexplore.ieee.org/document/6718113">VGG</a>]: <em>Simonyan, K., Vedaldi, A. and Zisserman, A., 2014. Learning local feature descriptors using convex optimisation. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>36</em>(8), pp.1573-1585.</em></p>
<h2 id="sift_1">SIFT</h2>
<p><em>Scale Invariant Feature Transform</em>
Image gradients are used  to describe the detected keypoints. Magnitudes and orientations are calculated for 16 subregions around the keypoint on a specific scale and for eight orientations and stored in a 128 element non-binary vector. By normalizing the values,  invariance under certain illumination changes is reassured.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://link.springer.com/article/10.1023/B:VISI.0000029664.99615.94">SIFT</a>]: <em>Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. International journal of computer vision, Vol. 60(2), pp.91-110.</em></p>
<h2 id="surf_1">SURF</h2>
<p><em>Speeded-Up Robust Features</em></p>
<p>SURF describes the detected features using Haar wavelets. Responses are calculated for 16 suregions around the detected feature, which are further subdivided, yielding a 64 element descriptor vector for each detected feature. In a similar fashion with SIFT, illumination invariance is achieved. SURF outperforms SIFT in terms of computational complexity.</p>
<p>[Source:self-adjusted]</p>
<p>[<a href="https://link.springer.com/chapter/10.1007/11744023_32">SURF</a>]: <em>Bay, H., Tuytelaars, T. and Van Gool, L., 2006. Surf: Speeded up robust features. In European conference on computer vision, pp. 404-417. Springer, Berlin, Heidelberg.</em></p>
<blockquote>
<p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p>
</blockquote>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Feature Matching/" class="btn btn-neutral float-right" title="Feature Matching">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href=".." class="btn btn-neutral" title="PhotoMatch"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href=".." style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../Feature Matching/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
